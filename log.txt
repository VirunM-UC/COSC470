Excel Dataset: 1484 records.
Structure type - attached: 230 (18%), semi-detached: 102 (8%), detached: 941 (73%)
Land use - commercial: 243, residential: 1101, industrial: 36, natural: 31, agricultural: 22, mixed: 22, excluded: 29 (empty, "other", etc)
Building material - cinder: 1098, brick: 206, steel: 20, construction: 48 (including combinations), wood: 32, mixed: 54 (excluding "construction"), excluded: 26 (empty)
Roof type - concrete: 636, corrugated: 240, tile: 471, shingle: 61, gravel: 9, brick: 4, thatched: 3, tar: 3, unknown/unable_to_determine: 20, other (combinations, one-item classes, carp): ?
Street type - paved: 868, cobble: 352, unfinished: 125, potholes: 118, other (combinations): ?

Started with structure_type because it had clean unordered classes with each data point having one unique class. Also it seems the most fruitful for visual classification.

Chose to store images and data together for easy loading. Used Pandas Dataframe and pickled it.

Switched from pure TensorFlow to Huggingface because ViT is not a default model in TF so loading it would have required me to learn more anyway, so I'd rather learn something higher level.

For missing images, save white squares and save missing image mask as csv file. In create_datasets.py, remove missing records and save.
MISSING = [107, 113, 363, 471, 480, 499, 588, 593, 597, 969, 1411, 1422, 1423, 1426, 1434, 1435, 1438, 1440, 1442, 1444, 1464, 1472, 1475, 1476]

ViT is classifying everything as detached (Most common class). Dataset is very imbalanced. Implemented upsampling to fix this. Class Weights did nothing.
Couldn't find upsampling method in Huggingface. Using Pandas Upsampling method. Fixed this. Accuracy at 50% (image memory inefficiency meant I used less data)

Switched from storing images as ND-arrays to PIL objects because former was too memory expensive (for full dataset) and would kill program in the DataFrame to HF dataset transfer.

Survey:
A frequent problem is unbalanced datasets.
A frequent practice is segmenting before classifying.


I made structure_type and building_conditions different scripts.
If it was just a matter of running uncustomised models on the data, then it would make sense to just have one script or one script with a data processing module.
However, if we want to customize based on each attribute, it's probably best to have different scripts.

building_conditions was chosen next because it had the least imbalanced data. It is ordinal data though.
Accuracy is 34%, without upsampling. Still didn't classify anything to 3 of the 5 classes.

With upsampling, building_conditions accuracy goes to 28%, but at least it's using all the classes. 
This might be too hard to classify on. Maybe it's hard to tell the difference between good and very good, and the visual features may be subtle in general.

Switching to pytorch because more models are implemented there for huggingface.

"Could not infer dtype of JpegImageFile" error. In the DataCollator.
Probably a version control error.
I think the DataCollator is trying to infer the dtype but torch cannot process PIL images. It's doing this inference without running the transforms.
Could try converting dataset to torch tensors.
Fixed. Somehow changing with_transforms to set_transforms fixed it.

Pytorch structure_type.py got 74% for some reason. Maybe because it used the full dataset. 
Precision: 0-48%, 1-50%, 2-80%
Overfitting after 2nd epoch.

TensorFlow structure_type on full dataset got 66%. Heavy overfitting after 2nd epoch.
Precision: 0-26%, 1-0%, 2-73%
Could be hyperparameter differences.

Trying Swin Transformer V2 on structure_type. 
Error: "RuntimeError: Error(s) in loading state_dict for Linear:
        size mismatch for weight: copying a param with shape torch.Size([1000, 1024]) from checkpoint, the shape in current model is torch.Size([3, 1024])."
.from_pretrained() seems to be loading the FFN classifier head which is a size mismatch with the 3 class output classifier head for this task. It shouldn't do that.
Fixed with ignore_mismatch_sizes = True.

SwinV2 on structure_type.
'eval_loss': 0.82, 'eval_accuracy': 0.72, 'eval_precision_0': 0.5, 'eval_precision_1': 0.16, 'eval_precision_2': 0.79, 'eval_recall': 0.47

These metrics are poor. I'm going to switch to using F1-score.

PyTorch:
ViT on structure_type:
'eval_accuracy': 0.71, 'eval_f1_0': 0.47, 'eval_f1_1': 0.0, 'eval_f1_2': 0.84
SwinV2 on structure_type:
'eval_accuracy': 0.73, 'eval_f1_0': 0.41, 'eval_f1_1': 0.14, 'eval_f1_2': 0.83
ConvNeXT on structure_type:
'eval_accuracy': 0.66, 'eval_f1_0': 0.43, 'eval_f1_1': 0.05, 'eval_f1_2': 0.79

-
ViT on building_conditions:
'eval_accuracy': 0.38, 'eval_f1_0': 0.18 'eval_f1_1': 0.31, 'eval_f1_2': 0.22, 'eval_f1_3': 0.36, 'eval_f1_4': 0.54
SwinV2 on building_conditions:
'eval_accuracy': 0.38, 'eval_f1_0': 0.07, 'eval_f1_1': 0.26, 'eval_f1_2': 0.16, 'eval_f1_3': 0.39, 'eval_f1_4': 0.56
ConvNeXT on building_conditions:
'eval_accuracy': 0.36, 'eval_f1_0': 0.15, 'eval_f1_1': 0.26, 'eval_f1_2': 0.29, 'eval_f1_3': 0.34, 'eval_f1_4': 0.50
-
ViT on landuse:
'eval_accuracy': 0.82, 'eval_f1_0': 0.90, 'eval_f1_1': 0.4
SwinV2 on landuse:
'eval_accuracy': 0.81, 'eval_f1_0': 0.89, 'eval_f1_1': 0.4
ConvNeXT on landuse:
'eval_accuracy': 0.82, 'eval_f1_0': 0.89, 'eval_f1_1': 0.40


In landuse, given the small number of records for most classes, I decided to just train it on residential and commercial classes. Pretty low F1-Score regardless.

Changed dataset column dtypes from category to object (for most columns) because the Categories contained garbage that messed with the map method. 
It was passing the garbage to the label2id dictionary.

SwinV2 and ConvNeXT only run in postgrad labs.

Created inference.py to produce a spreadsheet of the predicted and actual structure_type values, along with their images.
I figured that was the easiest way to analyze the results visually.

Also made it produce a confusion matrix.

Implemented stratified sampling.
-
Total - Accuracy: 0.718
bangkok - Accuracy: 1.000
britishcolumbia - Accuracy: 0.917
copenhagen - Accuracy: 1.000
dhaka - Accuracy: 0.929
durban - Accuracy: 0.900
harriscounty - Accuracy: 0.938
jakarta - Accuracy: 0.750
kualalumpur - Accuracy: 0.700
manila - Accuracy: 0.667
mocoa - Accuracy: 0.333
phnompenh - Accuracy: 1.000
queensland - Accuracy: 1.000
queretaro - Accuracy: 0.636
quito - Accuracy: 0.552
rajshahi - Accuracy: 0.562
sliven - Accuracy: 0.800

Loss is defined in the model.

Turned building_conditions into a regression problem.
Accuracy went down from 38% to 25%.
Outputs are bunching up around the average. I don't know how to choose the boundary values for classifying.
After choosing better ones I got:
'eval_accuracy': 0.40, macro f1: 0.23 compared to previous acc: 0.38, macro f1: 0.26

A big issue for structure_type is the building not being in frame.
Potential solution to this is making a composite image with different headings so that you can catch more of the building.

Some buildings have indoor images (useless). Change GSV parameters to only get outdoor ones.

Collected composite images.
MISSING = [60, 79, 107, 113, 363, 480, 499, 588, 593, 597, 924, 961, 969, 1411, 1422, 1423, 1426, 1434, 1435, 1438, 1440, 1442, 1444, 1464, 1469, 1472, 1475, 1476]

ViT on structure_type with composite images:
'eval_accuracy': 0.75, 'eval_f1_0': 0.54, 'eval_f1_1': 0.0
compare with regular images:
'eval_accuracy': 0.71, 'eval_f1_0': 0.47, 'eval_f1_1': 0.0

pickled dataframes don't store the compressed image files themselves.
They store links to the images.
If you move the image folder, they will fail.

ViT on building_conditions with composite images (not ordinal):
'eval_accuracy': 0.39, 'eval_f1_0': 0.14, 'eval_f1_1': 0.2, 'eval_f1_2': 0.32, 'eval_f1_3': 0.38, 'eval_f1_4': 0.54
compare with regular images:
'eval_accuracy': 0.38, 'eval_f1_0': 0.18 'eval_f1_1': 0.31, 'eval_f1_2': 0.22, 'eval_f1_3': 0.36, 'eval_f1_4': 0.54

Implemented building_material (only cinder and brick)
'eval_accuracy': 0.875, 'eval_f1_1': 0.51
Interesting that it's so poor. Arguably this should be the easiest attribute.
On composite images it's better:
'eval_accuracy': 0.90, 'eval_f1_1': 0.62

Slight Acc drop with ordinal building conditions on composite images. The output spread may have changed with the composite images.


Double data:
Map tiles API.
-call on location, get PanoIDs.
-call on each PanoID to get its coordinates and calculate distance from location. Then select closest PanoID and calculate heading.
Streetview API. Collect images. Only outdoor ones. (If using indoor panoID and source = outdoor, does it return an error?)
-
Got them.
Missing: 46 out of 1484 (3%)
[60, 79, 102, 107, 110, 113, 144, 149, 181, 191, 317, 363, 480, 499, 588, 593, 597, 612, 633, 634, 635, 742, 743, 784, 859, 924, 961, 969, 1015, 1043, 1124, 1411, 1422, 1423, 1434, 1435, 1438, 1440, 1442, 1444, 1461, 1464, 1469, 1472, 1475, 1476]

ViT on structure_type with images from multi panoramas:
'eval_accuracy': 0.77, 'eval_f1_0': 0.48, 'eval_f1_1': 0.10
ViT on structure_type with composite images:
'eval_accuracy': 0.75, 'eval_f1_0': 0.54, 'eval_f1_1': 0.0
compare with regular images:
'eval_accuracy': 0.71, 'eval_f1_0': 0.47, 'eval_f1_1': 0.0


Worth trying attributes that have higher interrater reliability. Apparently structure_type is low.
Need to check roof type classes. Street type is also a good idea. Occupancy status weirdly has a decent score, maybe because of the undetermined class.
Floors needs more research, but I could do some preliminary results.
Drains as 2 classes (present or not) is also worth checking.

Tried 2-class building_material on multi pano images.
'eval_accuracy': 0.90, 'eval_f1_1': 0.62
compared to 'eval_f1_1': 0.51 on regular images and 'eval_f1_1': 0.62 on composite. Interesting that it does as well as composite.
-
masking may improve the results.
-
An option is to have the classes be cinder, brick and other, which gives a bit more info while still avoiding the tiny classes.


Notes on Datasets:
NYC database - https://www.kaggle.com/datasets/new-york-city/nyc-buildings?select=PLUTODD16v2.pdf
Has land use, detached vs attached and semi-detached for residential, number of floors.

OSM collected database - https://www.nature.com/articles/s41597-024-04046-w#Abs1
EUBUCCO - https://www.nature.com/articles/s41597-023-02040-2#Sec20
Has residential vs non-residential

French databases
- https://data.europa.eu/data/datasets/61dc7157488f8cdb4283e3c3~~1?locale=en
- https://data.europa.eu/data/datasets/64f8681944e2fc006a93e65b?locale=en
Has material, roof type, prob more.
Dpt 1 has 300k records. Total dataset has 20 million.
Meterials - Stone, millstone, brick, wood, cinder (concrete), agglomerate, other, undetermined
cinder: 15k, brick: 9k, wood: 3k
Roof type - tiles (tuiles), concrete (beton),  shingles (Slates, ardoises), zinc aluminium, other, undetermined
tiles: 197k, concrete: 3k, shingles: 2k, zinc aluminium: 991
Dpt 75 (Paris) has 425k records.
cinder: 30k, brick: 25k, wood: 500
tiles: 27k, concrete: 39k, shingles: 15k, zinc aluminium: 125k

ViT on roof_type:
'eval_accuracy': 0.53, "eval_macro_f1": 0.49
Adding remote sensing would be good.

ViT on street_description:
'eval_f1_paved': 0.80, 'eval_f1_cobble': 0.72, 'eval_f1_unfinished': 0.47, 'eval_f1_potholes': 0.11
Potholes is likely to be bad because we don't have a picture of the full road. Don't know why unfinished is so low though.


TRAINING ON FRENCH DATA
CSV file.
A lot of the buildings do not have street view coverage.
Lots of occlusion.
Duplicate records. Fixed with SELECT DISTINCT
-
Note: French data is not only more numerous, but I collected a lower fov as well, so that might have an effect.
-
Missing: 1381 out of 7500 (18%)
-
'eval_accuracy': 0.59, 'eval_f1_macro': 0.59, 'eval_f1_beton': 0.60, 'eval_f1_briques': 0.57, 'eval_f1_bois': 0.61
compare with 'eval_f1_brick': 0.51 (training on only cinder and brick), on composite images that went to 0.62
Technically better than both since we have more classes now, but this is still quite low.
It would likely increase if we did composite images, but I think there's a smarter way to get more of the building in the image.

I collect images with max fov. It would be better to zoom in more into the building to get more building relevant data (especially for material).
However, since the images aren't guaranteed to be pointing at the building, I need a high fov so that the building is more likely to be in frame.
Additionally, we may need an image of the whole building (e.g.- for structure type) and the panorama is too close.
-
Maybe it would be better to collect default fov pictures (fov=90).
-
TRAINING ON PARIS DATA
The parcelle table that contains lat and lon only contains 77k of the 425k records.
-
Missing: 222 out of 8456 (3%)
-
'eval_accuracy': 0.81, 'eval_f1_macro': 0.81, 'eval_f1_beton': 0.79, 'eval_f1_briques': 0.83
'eval_loss': 0.450 vs 'loss': 0.234
Positive result. Overfitting considerably. Probably a good solution for that is more data. 
It took 80 minutes to collect 8k images (100/minute) which resulted in a training dataset of 5k. It took 7 minutes to train.
55k would take 550 minutes (9h10m) and would result in a training dataset of 33k. It would take 46 minutes to train (on a lab 3 computer)
-
Inference on global dataset
global - accuracy: 0.480, f1_macro: 0.409, f1_cinder: 0.614, f1_brick: 0.204
/
bangkok - accuracy: 0.700, f1_macro: 0.412, f1_cinder: 0.824, f1_brick: 0.000
britishcolumbia - accuracy: 0.700, f1_macro: 0.600, f1_cinder: 0.800, f1_brick: 0.400
copenhagen - accuracy: 0.312, f1_macro: 0.287, f1_cinder: 0.154, f1_brick: 0.421
dhaka - accuracy: 0.615, f1_macro: 0.381, f1_cinder: 0.762, f1_brick: 0.000
durban - accuracy: 0.333, f1_macro: 0.250, f1_cinder: 0.500, f1_brick: 0.000
harriscounty - accuracy: 0.385, f1_macro: 0.350, f1_cinder: 0.200, f1_brick: 0.500
jakarta - accuracy: 0.357, f1_macro: 0.263, f1_cinder: 0.526, f1_brick: 0.000
kualalumpur - accuracy: 0.100, f1_macro: 0.091, f1_cinder: 0.182, f1_brick: 0.000
manila - accuracy: 0.500, f1_macro: 0.333, f1_cinder: 0.667, f1_brick: 0.000
mocoa - accuracy: 0.500, f1_macro: 0.500, f1_cinder: 0.500, f1_brick: 0.500
phnompenh - accuracy: 0.429, f1_macro: 0.300, f1_cinder: 0.600, f1_brick: 0.000
queensland - accuracy: 0.444, f1_macro: 0.308, f1_cinder: 0.615, f1_brick: 0.000
queretaro - accuracy: 0.273, f1_macro: 0.267, f1_cinder: 0.200, f1_brick: 0.333
quito - accuracy: 0.516, f1_macro: 0.340, f1_cinder: 0.681, f1_brick: 0.000
rajshahi - accuracy: 0.818, f1_macro: 0.771, f1_cinder: 0.875, f1_brick: 0.667
sliven - accuracy: 0.500, f1_macro: 0.495, f1_cinder: 0.444, f1_brick: 0.545
-
The fov of the global dataset is still 120. I could change it but then I would have to rerun everything to get relevant comparisons.
All the other results are on fov 120.
I could also do a second round of training with the global dataset.
-
fov=90 inference - 
global - accuracy: 0.488, f1_macro: 0.414, f1_cinder: 0.623, f1_brick: 0.205
/
harriscounty - accuracy: 0.692, f1_macro: 0.639, f1_cinder: 0.500, f1_brick: 0.778
copenhagen - accuracy: 0.500, f1_macro: 0.333, f1_cinder: 0.000, f1_brick: 0.667
mocoa - accuracy: 0.400, f1_macro: 0.400, f1_cinder: 0.400, f1_brick: 0.400
rajshahi - accuracy: 0.600, f1_macro: 0.524, f1_cinder: 0.714, f1_brick: 0.333
kualalumpur - accuracy: 0.417, f1_macro: 0.378, f1_cinder: 0.533, f1_brick: 0.222
britishcolumbia - accuracy: 0.300, f1_macro: 0.231, f1_cinder: 0.462, f1_brick: 0.000
dhaka - accuracy: 0.357, f1_macro: 0.263, f1_cinder: 0.526, f1_brick: 0.000
queensland - accuracy: 0.500, f1_macro: 0.333, f1_cinder: 0.667, f1_brick: 0.000
quito - accuracy: 0.484, f1_macro: 0.326, f1_cinder: 0.652, f1_brick: 0.000
sliven - accuracy: 0.400, f1_macro: 0.286, f1_cinder: 0.571, f1_brick: 0.000
- cities with no brick buildings in validation set
(X) bangkok - accuracy: 0.700, f1_macro: 0.412, f1_cinder: 0.824, f1_brick: 0.000
(X) durban - accuracy: 0.500, f1_macro: 0.333, f1_cinder: 0.667, f1_brick: 0.000
(X) jakarta - accuracy: 0.467, f1_macro: 0.318, f1_cinder: 0.636, f1_brick: 0.000
(X) manila - accuracy: 0.727, f1_macro: 0.421, f1_cinder: 0.842, f1_brick: 0.000
(X) phnompenh - accuracy: 0.500, f1_macro: 0.333, f1_cinder: 0.667, f1_brick: 0.000
(X) queretaro - accuracy: 0.273, f1_macro: 0.214, f1_cinder: 0.429, f1_brick: 0.000
-
Compare with inference results of model trained on global dataset:
global - accuracy: 0.919, f1_macro: 0.799, f1_cinder: 0.954, f1_brick: 0.644
/
harriscounty - accuracy: 1.000, f1_macro: 1.000, f1_cinder: 1.000, f1_brick: 1.000
copenhagen - accuracy: 0.571, f1_macro: 0.562, f1_cinder: 0.500, f1_brick: 0.625
britishcolumbia - accuracy: 0.800, f1_macro: 0.688, f1_cinder: 0.875, f1_brick: 0.500
queensland - accuracy: 0.600, f1_macro: 0.583, f1_cinder: 0.667, f1_brick: 0.500
dhaka - accuracy: 0.857, f1_macro: 0.462, f1_cinder: 0.923, f1_brick: 0.000
kualalumpur - accuracy: 0.833, f1_macro: 0.455, f1_cinder: 0.909, f1_brick: 0.000
mocoa - accuracy: 0.800, f1_macro: 0.444, f1_cinder: 0.889, f1_brick: 0.000
quito - accuracy: 0.989, f1_macro: 0.497, f1_cinder: 0.995, f1_brick: 0.000
rajshahi - accuracy: 0.900, f1_macro: 0.474, f1_cinder: 0.947, f1_brick: 0.000
sliven - accuracy: 0.900, f1_macro: 0.474, f1_cinder: 0.947, f1_brick: 0.000
(X) bangkok - accuracy: 1.000, f1_macro: 0.500, f1_cinder: 1.000, f1_brick: 0.000
(X) durban - accuracy: 0.900, f1_macro: 0.474, f1_cinder: 0.947, f1_brick: 0.000
(X) jakarta - accuracy: 1.000, f1_macro: 0.500, f1_cinder: 1.000, f1_brick: 0.000
(X) manila - accuracy: 1.000, f1_macro: 0.500, f1_cinder: 1.000, f1_brick: 0.000
(X) phnompenh - accuracy: 1.000, f1_macro: 0.500, f1_cinder: 1.000, f1_brick: 0.000
(X) queretaro - accuracy: 1.000, f1_macro: 0.500, f1_cinder: 1.000, f1_brick: 0.000
-
The global model seems to do worse on non-european cities than the paris model.


Object detection
-
COCO dataset which is the premier dataset for object detection does not have a building class. Likely, I will have to use zero shot object detection.
-
maybe I can object detect on the whole pano to find the building and then get a picture specifically of that area.
I could also object detect it in the regular image and mask out irrelevant details.
-
On normal images (building_material), cropped:
'eval_f1_macro': 0.56, 'eval_f1_cinder': 0.91, 'eval_f1_brick': 0.20
Much worse than baseline. Not sure about my selection criteria for boxes. Also there was a loss of 100 images that didn't have predictions.
I also cropped instead of masked. Don't know if that made a difference. Maybe the resizing made it harder to tell.
masked:
eval_f1_macro': 0.45, 'eval_f1_cinder': 0.91, 'eval_f1_brick': 0.0
masked is even worse.

Working on semantic segmentation.
'eval_f1_macro': 0.45, 'eval_f1_cinder': 0.91, 'eval_f1_brick': 0.0
Also nothing.

I am testing whether the default fov would do better for building_material.
I have collected the dataset. Randomly, the validation set has 13% brick compared to 19% in the full dataset. 
I could stratify based on material but I am already stratifying based on city.

Everything attempted on building_material (cinder and brick) on global dataset:
fov=120                                                         fov=90
ViT - 'eval_f1_1': 0.51                                         'eval_f1_brick': 0.64
ViT and composite images - 'eval_f1_1': 0.62
ViT and multi pano images - 'eval_f1_1': 0.62
ViT and bounding boxes (cropped) - 'eval_f1_brick': 0.20
ViT and bounding boxes (masked) - 'eval_f1_brick': 0.0
ViT and semantic segmentation - 'eval_f1_brick': 0.0
ViT and trained on paris data - f1_brick: 0.204


Tests:
see how well object detection works on the equirectangular projection

TODO:
Change global images to fov 90 and source = outdoor and retrain everything. Collect results in a table.
Need to also analyse building condition outputs and landuse outputs.
Need to implement more attributes.
